{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fastaudio An audio module for fastai v2. We want to help you build audio machine learning applications while minimizing the need for audio domain expertise. Currently under development. Quick Start Google Colab Notebook Zachary Mueller's class","title":"Fastaudio"},{"location":"#fastaudio","text":"An audio module for fastai v2. We want to help you build audio machine learning applications while minimizing the need for audio domain expertise. Currently under development.","title":"Fastaudio"},{"location":"#quick-start","text":"Google Colab Notebook Zachary Mueller's class","title":"Quick Start"},{"location":"Training_tutorial/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Simple training tutorial The objective of this tutorial is to show you the basics of the library and how it can be used to simplify the audio processing pipeline. This page is generated from the corresponding jupyter notebook, that can be found on this folder To install the library, just run this cell: ! pip install git + https : // github . com / fastaudio / fastaudio . git COLAB USERS: Before you continue and import the lib, go to the Runtime menu and select Restart Runtime . from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * ESC-50: Dataset for Environmental Sound Classification #The first time this will download a dataset that is ~650mb path = untar_data ( URLs . ESC50 , dest = \"ESC50\" ) The audio files are inside a subfolder audio/ ( path / \"audio\" ) . ls () (#2000) [Path('/home/scart/.fastai/data/master/audio/5-198891-A-8.wav'),Path('/home/scart/.fastai/data/master/audio/3-128512-A-47.wav'),Path('/home/scart/.fastai/data/master/audio/4-234879-A-6.wav'),Path('/home/scart/.fastai/data/master/audio/3-100024-A-27.wav'),Path('/home/scart/.fastai/data/master/audio/5-263831-A-6.wav'),Path('/home/scart/.fastai/data/master/audio/1-22804-A-46.wav'),Path('/home/scart/.fastai/data/master/audio/2-117615-A-48.wav'),Path('/home/scart/.fastai/data/master/audio/5-221518-A-21.wav'),Path('/home/scart/.fastai/data/master/audio/2-43802-A-42.wav'),Path('/home/scart/.fastai/data/master/audio/5-194899-D-3.wav')...] And there's another folder meta/ with some metadata about all the files and the labels ( path / \"meta\" ) . ls () (#2) [Path('/home/scart/.fastai/data/master/meta/esc50.csv'),Path('/home/scart/.fastai/data/master/meta/esc50-human.xlsx')] Opening the metadata file df = pd . read_csv ( path / \"meta\" / \"esc50.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } filename fold target category esc10 src_file take 0 1-100032-A-0.wav 1 0 dog True 100032 A 1 1-100038-A-14.wav 1 14 chirping_birds False 100038 A 2 1-100210-A-36.wav 1 36 vacuum_cleaner False 100210 A 3 1-100210-B-36.wav 1 36 vacuum_cleaner False 100210 B 4 1-101296-A-19.wav 1 19 thunderstorm False 101296 A Datablock and Basic End to End Training # Helper function to split the data def CrossValidationSplitter ( col = 'fold' , fold = 1 ): \"Split `items` (supposed to be a dataframe) by fold in `col`\" def _inner ( o ): assert isinstance ( o , pd . DataFrame ), \"ColSplitter only works when your items are a pandas DataFrame\" col_values = o . iloc [:, col ] if isinstance ( col , int ) else o [ col ] valid_idx = ( col_values == fold ) . values . astype ( 'bool' ) return IndexSplitter ( mask2idxs ( valid_idx ))( o ) return _inner Creating the Audio to Spectrogram transform from a predefined config. cfg = AudioConfig . BasicMelSpectrogram ( n_fft = 512 ) a2s = AudioToSpec . from_cfg ( cfg ) Creating the Datablock auds = DataBlock ( blocks = ( AudioBlock , CategoryBlock ), get_x = ColReader ( \"filename\" , pref = path / \"audio\" ), splitter = CrossValidationSplitter ( fold = 1 ), batch_tfms = [ a2s ], get_y = ColReader ( \"category\" )) dbunch = auds . dataloaders ( df , bs = 64 ) Visualizing one batch of data. Notice that the title of each Spectrogram is the corresponding label. dbunch . show_batch ( figsize = ( 10 , 5 )) Learner and Training While creating the learner, we need to pass a special cnn_config to indicate that our input spectrograms only have one channel. Besides that, it's the usual vision learner. learn = cnn_learner ( dbunch , resnet18 , config = cnn_config ( n_in = 1 ), #<- Only audio specific modification here loss_fn = CrossEntropyLossFlat , metrics = [ accuracy ]) learn . fine_tune ( 10 ) epoch train_loss valid_loss accuracy time 0 4.559088 2.335686 0.340000 00:09 epoch train_loss valid_loss accuracy time 0 2.299654 1.825524 0.497500 00:09 1 1.779437 1.425651 0.590000 00:09 2 1.301816 1.242665 0.642500 00:09 3 0.882593 1.107010 0.692500 00:09 4 0.596167 1.014652 0.707500 00:09 5 0.401981 0.997006 0.720000 00:09 6 0.282515 1.003401 0.727500 00:09 7 0.197506 0.973345 0.740000 00:09 8 0.144901 0.970970 0.727500 00:09 9 0.108945 0.977597 0.727500 00:09","title":"Training tutorial"},{"location":"Training_tutorial/#simple-training-tutorial","text":"The objective of this tutorial is to show you the basics of the library and how it can be used to simplify the audio processing pipeline. This page is generated from the corresponding jupyter notebook, that can be found on this folder To install the library, just run this cell: ! pip install git + https : // github . com / fastaudio / fastaudio . git COLAB USERS: Before you continue and import the lib, go to the Runtime menu and select Restart Runtime . from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import *","title":"Simple training tutorial"},{"location":"Training_tutorial/#esc-50-dataset-for-environmental-sound-classification","text":"#The first time this will download a dataset that is ~650mb path = untar_data ( URLs . ESC50 , dest = \"ESC50\" ) The audio files are inside a subfolder audio/ ( path / \"audio\" ) . ls () (#2000) [Path('/home/scart/.fastai/data/master/audio/5-198891-A-8.wav'),Path('/home/scart/.fastai/data/master/audio/3-128512-A-47.wav'),Path('/home/scart/.fastai/data/master/audio/4-234879-A-6.wav'),Path('/home/scart/.fastai/data/master/audio/3-100024-A-27.wav'),Path('/home/scart/.fastai/data/master/audio/5-263831-A-6.wav'),Path('/home/scart/.fastai/data/master/audio/1-22804-A-46.wav'),Path('/home/scart/.fastai/data/master/audio/2-117615-A-48.wav'),Path('/home/scart/.fastai/data/master/audio/5-221518-A-21.wav'),Path('/home/scart/.fastai/data/master/audio/2-43802-A-42.wav'),Path('/home/scart/.fastai/data/master/audio/5-194899-D-3.wav')...] And there's another folder meta/ with some metadata about all the files and the labels ( path / \"meta\" ) . ls () (#2) [Path('/home/scart/.fastai/data/master/meta/esc50.csv'),Path('/home/scart/.fastai/data/master/meta/esc50-human.xlsx')] Opening the metadata file df = pd . read_csv ( path / \"meta\" / \"esc50.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } filename fold target category esc10 src_file take 0 1-100032-A-0.wav 1 0 dog True 100032 A 1 1-100038-A-14.wav 1 14 chirping_birds False 100038 A 2 1-100210-A-36.wav 1 36 vacuum_cleaner False 100210 A 3 1-100210-B-36.wav 1 36 vacuum_cleaner False 100210 B 4 1-101296-A-19.wav 1 19 thunderstorm False 101296 A","title":"ESC-50: Dataset for Environmental Sound Classification"},{"location":"Training_tutorial/#datablock-and-basic-end-to-end-training","text":"# Helper function to split the data def CrossValidationSplitter ( col = 'fold' , fold = 1 ): \"Split `items` (supposed to be a dataframe) by fold in `col`\" def _inner ( o ): assert isinstance ( o , pd . DataFrame ), \"ColSplitter only works when your items are a pandas DataFrame\" col_values = o . iloc [:, col ] if isinstance ( col , int ) else o [ col ] valid_idx = ( col_values == fold ) . values . astype ( 'bool' ) return IndexSplitter ( mask2idxs ( valid_idx ))( o ) return _inner Creating the Audio to Spectrogram transform from a predefined config. cfg = AudioConfig . BasicMelSpectrogram ( n_fft = 512 ) a2s = AudioToSpec . from_cfg ( cfg ) Creating the Datablock auds = DataBlock ( blocks = ( AudioBlock , CategoryBlock ), get_x = ColReader ( \"filename\" , pref = path / \"audio\" ), splitter = CrossValidationSplitter ( fold = 1 ), batch_tfms = [ a2s ], get_y = ColReader ( \"category\" )) dbunch = auds . dataloaders ( df , bs = 64 ) Visualizing one batch of data. Notice that the title of each Spectrogram is the corresponding label. dbunch . show_batch ( figsize = ( 10 , 5 ))","title":"Datablock and Basic End to End Training"},{"location":"Training_tutorial/#learner-and-training","text":"While creating the learner, we need to pass a special cnn_config to indicate that our input spectrograms only have one channel. Besides that, it's the usual vision learner. learn = cnn_learner ( dbunch , resnet18 , config = cnn_config ( n_in = 1 ), #<- Only audio specific modification here loss_fn = CrossEntropyLossFlat , metrics = [ accuracy ]) learn . fine_tune ( 10 ) epoch train_loss valid_loss accuracy time 0 4.559088 2.335686 0.340000 00:09 epoch train_loss valid_loss accuracy time 0 2.299654 1.825524 0.497500 00:09 1 1.779437 1.425651 0.590000 00:09 2 1.301816 1.242665 0.642500 00:09 3 0.882593 1.107010 0.692500 00:09 4 0.596167 1.014652 0.707500 00:09 5 0.401981 0.997006 0.720000 00:09 6 0.282515 1.003401 0.727500 00:09 7 0.197506 0.973345 0.740000 00:09 8 0.144901 0.970970 0.727500 00:09 9 0.108945 0.977597 0.727500 00:09","title":"Learner and Training"},{"location":"TutorialSpeechCommands/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Training on Speech Commands If you havent installed fastaudio do it uncommenting and executing the following cell #!pip install git+https://github.com/fastaudio/fastaudio.git from fastai.torch_basics import * from fastai.basics import * from fastai.data.all import * from fastai.callback.all import * from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * import torchaudio Baseline The dataset is about 2.26 G r = torchaudio . datasets . SPEECHCOMMANDS ( \".\" , download = True ) r <torchaudio.datasets.speechcommands.SPEECHCOMMANDS at 0x7f87f456f160> commands_path = Path ( \"SpeechCommands\" ) audio_files = get_audio_files ( commands_path ) len ( audio_files ) 105835 for i in range ( 10 ): print ( random . choice ( audio_files )) SpeechCommands/speech_commands_v0.02/down/61abbf52_nohash_0.wav SpeechCommands/speech_commands_v0.02/three/773e26f7_nohash_2.wav SpeechCommands/speech_commands_v0.02/dog/73f20b00_nohash_0.wav SpeechCommands/speech_commands_v0.02/nine/72242187_nohash_0.wav SpeechCommands/speech_commands_v0.02/forward/3b195250_nohash_4.wav SpeechCommands/speech_commands_v0.02/eight/6ef407da_nohash_3.wav SpeechCommands/speech_commands_v0.02/go/4bc2c2c5_nohash_0.wav SpeechCommands/speech_commands_v0.02/three/c2d15ea5_nohash_0.wav SpeechCommands/speech_commands_v0.02/eight/a7200079_nohash_1.wav SpeechCommands/speech_commands_v0.02/down/3b195250_nohash_1.wav for i in range ( 10 ): f = random . choice ( audio_files ) print ( \"File:\" , f ) print ( \"Label:\" , parent_label ( f )) File: SpeechCommands/speech_commands_v0.02/eight/ff2b842e_nohash_2.wav Label: eight File: SpeechCommands/speech_commands_v0.02/left/20a0d54b_nohash_0.wav Label: left File: SpeechCommands/speech_commands_v0.02/one/e2008f39_nohash_0.wav Label: one File: SpeechCommands/speech_commands_v0.02/visual/3a3ee7ed_nohash_0.wav Label: visual File: SpeechCommands/speech_commands_v0.02/go/4a1e736b_nohash_2.wav Label: go File: SpeechCommands/speech_commands_v0.02/visual/72242187_nohash_0.wav Label: visual File: SpeechCommands/speech_commands_v0.02/dog/833a0279_nohash_0.wav Label: dog File: SpeechCommands/speech_commands_v0.02/off/563aa4e6_nohash_2.wav Label: off File: SpeechCommands/speech_commands_v0.02/bird/e102119e_nohash_0.wav Label: bird File: SpeechCommands/speech_commands_v0.02/five/c39703ec_nohash_0.wav Label: five DBMelSpec = SpectrogramTransformer ( mel = True , to_db = True ) a2s = DBMelSpec () crop_4000ms = CropSignal ( 4000 ) tfms = [ crop_4000ms , a2s ] auds = DataBlock ( blocks = ( AudioBlock , CategoryBlock ), get_items = get_audio_files , splitter = RandomSplitter (), item_tfms = tfms , get_y = parent_label ) audio_dbunch = auds . dataloaders ( commands_path , item_tfms = tfms , bs = 64 ) # credit to Kevin Bird and Hiromi Suenaga for these two lines to adjust a CNN model to take 1 channel input def alter_learner ( learn , channels = 1 ): learn . model [ 0 ][ 0 ] . in_channels = channels learn . model [ 0 ][ 0 ] . weight = torch . nn . parameter . Parameter ( learn . model [ 0 ][ 0 ] . weight [:, 1 ,:,:] . unsqueeze ( 1 )) learn = Learner ( audio_dbunch , xresnet18 (), torch . nn . CrossEntropyLoss (), metrics = [ accuracy ]) nchannels = audio_dbunch . one_batch ()[ 0 ] . shape [ 1 ] alter_learner ( learn , nchannels ) learn . lr_find () SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.0020892962347716093) learn . fit_one_cycle ( 5 , lr_max = slice ( 1e-2 )) epoch train_loss valid_loss accuracy time 0 0.656395 4.380031 0.299334 02:03 1 0.363153 0.380459 0.886852 02:00 2 0.252141 0.366696 0.890773 01:43 3 0.166282 0.195533 0.940993 01:40 4 0.117679 0.169219 0.950300 01:39 learn . lr_find () learn . unfreeze () learn . fit_one_cycle ( 5 , lr_max = slice ( 1e-3 )) epoch train_loss valid_loss accuracy time 0 0.108651 0.177195 0.948930 01:39 1 0.106599 0.174635 0.949119 01:36 2 0.100905 0.173525 0.951528 01:46 3 0.090713 0.168025 0.953182 01:50 4 0.081791 0.169523 0.954363 01:45 Customize our AudioToSpec Function using a config voice_cfg = AudioConfig . Voice () a2s = AudioToSpec . from_cfg ( voice_cfg ) tfms = [ crop_4000ms , a2s ] auds . item_tfms = tfms # tfms = Pipeline([CropSignal(4000), a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True) dbunch250B = auds . dataloaders ( commands_path , bs = 64 ) learn = Learner ( dbunch250B , xresnet18 (), torch . nn . CrossEntropyLoss (), metrics = [ accuracy ]) nchannels = dbunch250B . one_batch ()[ 0 ] . shape [ 1 ] alter_learner ( learn , nchannels ) learn . lr_find () SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.002511886414140463) # Better results even without fine tuning, but much slower. We need to move a2s to the GPU and # then add data augmentation! learn . fit_one_cycle ( 5 , lr_max = slice ( 2e-2 )) epoch train_loss valid_loss accuracy time 0 0.513826 0.857900 0.741343 04:42 1 0.278715 0.291171 0.912883 04:34 2 0.185014 0.192646 0.943119 04:33 3 0.136495 0.140570 0.957481 04:35 4 0.076968 0.124036 0.964048 04:36 Training an MFCC with Delta # only grab 1500ms of the clip, voice identity can be done with shorter sections and it will speed it up # this is really slow for mfcc, even for 45k files, need to figure out what's going on here. Also the results # shouldn't be this much worse than melspectrogram a2mfcc = AudioToMFCC ( n_mffc = 20 , melkwargs = { \"n_fft\" : 2048 , \"hop_length\" : 256 , \"n_mels\" : 128 }) tfms = [ CropSignal ( 1500 ), a2mfcc , Delta ()] auds . item_tfms = tfms # tfms = Pipeline([CropSignal(4000), a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True) dbunch_mfcc = auds . dataloaders ( commands_path , bs = 64 ) #n_mfcc isn't getting passed down? dbunch_mfcc . one_batch ()[ 0 ] . shape torch.Size([64, 3, 40, 94]) learn = Learner ( dbunch_mfcc , xresnet18 (), torch . nn . CrossEntropyLoss (), metrics = [ accuracy ]) learn . lr_find () SuggestedLRs(lr_min=0.15848932266235352, lr_steep=0.00363078061491251) learn . fit_one_cycle ( 5 , lr_max = slice ( 2e-2 )) epoch train_loss valid_loss accuracy time 0 0.558041 0.821212 0.769027 02:45 1 0.369136 0.475784 0.858553 02:40 2 0.275744 0.333747 0.899891 02:44 3 0.168651 0.176755 0.946048 02:42 4 0.125728 0.160911 0.952851 02:39 From Here: 1. Get transforms on the GPU 2. Once it's faster test signal and spectrogram augments for speed/efficacy 3. Fine-tune and see how high we can push results on 250 speakers","title":"TutorialSpeechCommands"},{"location":"TutorialSpeechCommands/#training-on-speech-commands","text":"If you havent installed fastaudio do it uncommenting and executing the following cell #!pip install git+https://github.com/fastaudio/fastaudio.git from fastai.torch_basics import * from fastai.basics import * from fastai.data.all import * from fastai.callback.all import * from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * import torchaudio","title":"Training on Speech Commands"},{"location":"TutorialSpeechCommands/#baseline","text":"The dataset is about 2.26 G r = torchaudio . datasets . SPEECHCOMMANDS ( \".\" , download = True ) r <torchaudio.datasets.speechcommands.SPEECHCOMMANDS at 0x7f87f456f160> commands_path = Path ( \"SpeechCommands\" ) audio_files = get_audio_files ( commands_path ) len ( audio_files ) 105835 for i in range ( 10 ): print ( random . choice ( audio_files )) SpeechCommands/speech_commands_v0.02/down/61abbf52_nohash_0.wav SpeechCommands/speech_commands_v0.02/three/773e26f7_nohash_2.wav SpeechCommands/speech_commands_v0.02/dog/73f20b00_nohash_0.wav SpeechCommands/speech_commands_v0.02/nine/72242187_nohash_0.wav SpeechCommands/speech_commands_v0.02/forward/3b195250_nohash_4.wav SpeechCommands/speech_commands_v0.02/eight/6ef407da_nohash_3.wav SpeechCommands/speech_commands_v0.02/go/4bc2c2c5_nohash_0.wav SpeechCommands/speech_commands_v0.02/three/c2d15ea5_nohash_0.wav SpeechCommands/speech_commands_v0.02/eight/a7200079_nohash_1.wav SpeechCommands/speech_commands_v0.02/down/3b195250_nohash_1.wav for i in range ( 10 ): f = random . choice ( audio_files ) print ( \"File:\" , f ) print ( \"Label:\" , parent_label ( f )) File: SpeechCommands/speech_commands_v0.02/eight/ff2b842e_nohash_2.wav Label: eight File: SpeechCommands/speech_commands_v0.02/left/20a0d54b_nohash_0.wav Label: left File: SpeechCommands/speech_commands_v0.02/one/e2008f39_nohash_0.wav Label: one File: SpeechCommands/speech_commands_v0.02/visual/3a3ee7ed_nohash_0.wav Label: visual File: SpeechCommands/speech_commands_v0.02/go/4a1e736b_nohash_2.wav Label: go File: SpeechCommands/speech_commands_v0.02/visual/72242187_nohash_0.wav Label: visual File: SpeechCommands/speech_commands_v0.02/dog/833a0279_nohash_0.wav Label: dog File: SpeechCommands/speech_commands_v0.02/off/563aa4e6_nohash_2.wav Label: off File: SpeechCommands/speech_commands_v0.02/bird/e102119e_nohash_0.wav Label: bird File: SpeechCommands/speech_commands_v0.02/five/c39703ec_nohash_0.wav Label: five DBMelSpec = SpectrogramTransformer ( mel = True , to_db = True ) a2s = DBMelSpec () crop_4000ms = CropSignal ( 4000 ) tfms = [ crop_4000ms , a2s ] auds = DataBlock ( blocks = ( AudioBlock , CategoryBlock ), get_items = get_audio_files , splitter = RandomSplitter (), item_tfms = tfms , get_y = parent_label ) audio_dbunch = auds . dataloaders ( commands_path , item_tfms = tfms , bs = 64 ) # credit to Kevin Bird and Hiromi Suenaga for these two lines to adjust a CNN model to take 1 channel input def alter_learner ( learn , channels = 1 ): learn . model [ 0 ][ 0 ] . in_channels = channels learn . model [ 0 ][ 0 ] . weight = torch . nn . parameter . Parameter ( learn . model [ 0 ][ 0 ] . weight [:, 1 ,:,:] . unsqueeze ( 1 )) learn = Learner ( audio_dbunch , xresnet18 (), torch . nn . CrossEntropyLoss (), metrics = [ accuracy ]) nchannels = audio_dbunch . one_batch ()[ 0 ] . shape [ 1 ] alter_learner ( learn , nchannels ) learn . lr_find () SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.0020892962347716093) learn . fit_one_cycle ( 5 , lr_max = slice ( 1e-2 )) epoch train_loss valid_loss accuracy time 0 0.656395 4.380031 0.299334 02:03 1 0.363153 0.380459 0.886852 02:00 2 0.252141 0.366696 0.890773 01:43 3 0.166282 0.195533 0.940993 01:40 4 0.117679 0.169219 0.950300 01:39 learn . lr_find () learn . unfreeze () learn . fit_one_cycle ( 5 , lr_max = slice ( 1e-3 )) epoch train_loss valid_loss accuracy time 0 0.108651 0.177195 0.948930 01:39 1 0.106599 0.174635 0.949119 01:36 2 0.100905 0.173525 0.951528 01:46 3 0.090713 0.168025 0.953182 01:50 4 0.081791 0.169523 0.954363 01:45","title":"Baseline"},{"location":"TutorialSpeechCommands/#customize-our-audiotospec-function-using-a-config","text":"voice_cfg = AudioConfig . Voice () a2s = AudioToSpec . from_cfg ( voice_cfg ) tfms = [ crop_4000ms , a2s ] auds . item_tfms = tfms # tfms = Pipeline([CropSignal(4000), a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True) dbunch250B = auds . dataloaders ( commands_path , bs = 64 ) learn = Learner ( dbunch250B , xresnet18 (), torch . nn . CrossEntropyLoss (), metrics = [ accuracy ]) nchannels = dbunch250B . one_batch ()[ 0 ] . shape [ 1 ] alter_learner ( learn , nchannels ) learn . lr_find () SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.002511886414140463) # Better results even without fine tuning, but much slower. We need to move a2s to the GPU and # then add data augmentation! learn . fit_one_cycle ( 5 , lr_max = slice ( 2e-2 )) epoch train_loss valid_loss accuracy time 0 0.513826 0.857900 0.741343 04:42 1 0.278715 0.291171 0.912883 04:34 2 0.185014 0.192646 0.943119 04:33 3 0.136495 0.140570 0.957481 04:35 4 0.076968 0.124036 0.964048 04:36","title":"Customize our AudioToSpec Function using a config"},{"location":"TutorialSpeechCommands/#training-an-mfcc-with-delta","text":"# only grab 1500ms of the clip, voice identity can be done with shorter sections and it will speed it up # this is really slow for mfcc, even for 45k files, need to figure out what's going on here. Also the results # shouldn't be this much worse than melspectrogram a2mfcc = AudioToMFCC ( n_mffc = 20 , melkwargs = { \"n_fft\" : 2048 , \"hop_length\" : 256 , \"n_mels\" : 128 }) tfms = [ CropSignal ( 1500 ), a2mfcc , Delta ()] auds . item_tfms = tfms # tfms = Pipeline([CropSignal(4000), a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True) dbunch_mfcc = auds . dataloaders ( commands_path , bs = 64 ) #n_mfcc isn't getting passed down? dbunch_mfcc . one_batch ()[ 0 ] . shape torch.Size([64, 3, 40, 94]) learn = Learner ( dbunch_mfcc , xresnet18 (), torch . nn . CrossEntropyLoss (), metrics = [ accuracy ]) learn . lr_find () SuggestedLRs(lr_min=0.15848932266235352, lr_steep=0.00363078061491251) learn . fit_one_cycle ( 5 , lr_max = slice ( 2e-2 )) epoch train_loss valid_loss accuracy time 0 0.558041 0.821212 0.769027 02:45 1 0.369136 0.475784 0.858553 02:40 2 0.275744 0.333747 0.899891 02:44 3 0.168651 0.176755 0.946048 02:42 4 0.125728 0.160911 0.952851 02:39 From Here: 1. Get transforms on the GPU 2. Once it's faster test signal and spectrogram augments for speed/efficacy 3. Fine-tune and see how high we can push results on 250 speakers","title":"Training an MFCC with Delta"},{"location":"TutorialViceRecognition/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Tutorial: Training a Voice Recognition Model\u00b6 If you havent installed fastaudio do it uncommenting and executing the following cell #!pip install git+https://github.com/fastaudio/fastaudio.git from fastai.torch_basics import * from fastai.basics import * from fastai.data.all import * from fastai.callback.all import * from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * import torchaudio p10speakers = untar_data ( URLs . SPEAKERS10 , extract_func = tar_extract_at_filename ) x = AudioGetter ( \"\" , recurse = True , folders = None ) files_10 = x ( p10speakers ) #original_aud = AudioItem.create(files[0]) Datablock and Basic End to End Training on 10 Speakers\u00b6 #crop 2s from the signal and turn it to a MelSpectrogram with no augmentation cfg_voice = AudioConfig . Voice () a2s = AudioToSpec . from_cfg ( cfg_voice ) auds = DataBlock ( blocks = ( AudioBlock . from_folder ( p10speakers , crop_signal_to = 2000 ), CategoryBlock ), get_items = get_audio_files , splitter = RandomSplitter (), item_tfms = a2s , get_y = lambda x : str ( x ) . split ( '/' )[ - 1 ][: 5 ]) cats = [ y for _ , y in auds . datasets ( p10speakers )] #verify categories are being correctly assigned test_eq ( min ( cats ) . item (), 0 ) test_eq ( max ( cats ) . item (), 9 ) dbunch = auds . dataloaders ( p10speakers , bs = 64 ) Info: Show batch is fixed now on nchannels, which is an object of AudioSpectrogram (part of sg settings but we overrode getattr to make it work like an attribute). dbunch . show_batch ( max_n = 9 ) /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The 'basey' parameter of __init__() has been renamed 'base' since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The 'linthreshy' parameter of __init__() has been renamed 'linthresh' since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) dbunch . one_batch ()[ 0 ] . shape torch.Size([64, 1, 128, 251]) # credit to Kevin Bird and Hiromi Suenaga for these two lines to adjust a CNN model to take 1 channel input def alter_learner ( learn , channels = 1 ): learn . model [ 0 ][ 0 ] . in_channels = channels learn . model [ 0 ][ 0 ] . weight = torch . nn . parameter . Parameter ( learn . model [ 0 ][ 0 ] . weight [:, 1 ,:,:] . unsqueeze ( 1 )) learn = Learner ( dbunch , xresnet18 (), torch . nn . CrossEntropyLoss (), metrics = [ accuracy ]) nchannels = dbunch . one_batch ()[ 0 ] . shape [ 1 ] alter_learner ( learn , nchannels ) learn . lr_find () SuggestedLRs(lr_min=0.010000000149011612, lr_steep=0.0030199517495930195) #epochs are a bit longer due to the chosen melspectrogram settings learn . fit_one_cycle ( 10 , lr_max = slice ( 1e-2 )) epoch train_loss valid_loss accuracy time 0 1.982005 1.652000 0.535156 00:06 1 0.813599 1.122795 0.726562 00:06 2 0.434651 1.328267 0.740885 00:06 3 0.266242 0.408208 0.839844 00:05 4 0.144300 0.181555 0.927083 00:06 5 0.082170 0.064481 0.979167 00:06 6 0.049960 0.847989 0.841146 00:06 7 0.033458 0.023514 0.994792 00:06 8 0.021735 0.016900 0.994792 00:05 9 0.015487 0.017779 0.996094 00:06","title":"TutorialViceRecognition"},{"location":"TutorialViceRecognition/#tutorial-training-a-voice-recognition-model","text":"If you havent installed fastaudio do it uncommenting and executing the following cell #!pip install git+https://github.com/fastaudio/fastaudio.git from fastai.torch_basics import * from fastai.basics import * from fastai.data.all import * from fastai.callback.all import * from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * import torchaudio p10speakers = untar_data ( URLs . SPEAKERS10 , extract_func = tar_extract_at_filename ) x = AudioGetter ( \"\" , recurse = True , folders = None ) files_10 = x ( p10speakers ) #original_aud = AudioItem.create(files[0])","title":"Tutorial: Training a Voice Recognition Model\u00b6"},{"location":"TutorialViceRecognition/#datablock-and-basic-end-to-end-training-on-10-speakers","text":"#crop 2s from the signal and turn it to a MelSpectrogram with no augmentation cfg_voice = AudioConfig . Voice () a2s = AudioToSpec . from_cfg ( cfg_voice ) auds = DataBlock ( blocks = ( AudioBlock . from_folder ( p10speakers , crop_signal_to = 2000 ), CategoryBlock ), get_items = get_audio_files , splitter = RandomSplitter (), item_tfms = a2s , get_y = lambda x : str ( x ) . split ( '/' )[ - 1 ][: 5 ]) cats = [ y for _ , y in auds . datasets ( p10speakers )] #verify categories are being correctly assigned test_eq ( min ( cats ) . item (), 0 ) test_eq ( max ( cats ) . item (), 9 ) dbunch = auds . dataloaders ( p10speakers , bs = 64 ) Info: Show batch is fixed now on nchannels, which is an object of AudioSpectrogram (part of sg settings but we overrode getattr to make it work like an attribute). dbunch . show_batch ( max_n = 9 ) /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The 'basey' parameter of __init__() has been renamed 'base' since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) /home/tyoc213/miniconda3/envs/fastai/lib/python3.8/site-packages/librosa/display.py:974: MatplotlibDeprecationWarning: The 'linthreshy' parameter of __init__() has been renamed 'linthresh' since Matplotlib 3.3; support for the old name will be dropped two minor releases later. scaler(mode, **kwargs) dbunch . one_batch ()[ 0 ] . shape torch.Size([64, 1, 128, 251]) # credit to Kevin Bird and Hiromi Suenaga for these two lines to adjust a CNN model to take 1 channel input def alter_learner ( learn , channels = 1 ): learn . model [ 0 ][ 0 ] . in_channels = channels learn . model [ 0 ][ 0 ] . weight = torch . nn . parameter . Parameter ( learn . model [ 0 ][ 0 ] . weight [:, 1 ,:,:] . unsqueeze ( 1 )) learn = Learner ( dbunch , xresnet18 (), torch . nn . CrossEntropyLoss (), metrics = [ accuracy ]) nchannels = dbunch . one_batch ()[ 0 ] . shape [ 1 ] alter_learner ( learn , nchannels ) learn . lr_find () SuggestedLRs(lr_min=0.010000000149011612, lr_steep=0.0030199517495930195) #epochs are a bit longer due to the chosen melspectrogram settings learn . fit_one_cycle ( 10 , lr_max = slice ( 1e-2 )) epoch train_loss valid_loss accuracy time 0 1.982005 1.652000 0.535156 00:06 1 0.813599 1.122795 0.726562 00:06 2 0.434651 1.328267 0.740885 00:06 3 0.266242 0.408208 0.839844 00:05 4 0.144300 0.181555 0.927083 00:06 5 0.082170 0.064481 0.979167 00:06 6 0.049960 0.847989 0.841146 00:06 7 0.033458 0.023514 0.994792 00:06 8 0.021735 0.016900 0.994792 00:05 9 0.015487 0.017779 0.996094 00:06","title":"Datablock and Basic End to End Training on 10 Speakers\u00b6"},{"location":"API_Reference/augment.preprocess/","text":"augment.preprocess RemoveType class fastaudio.augment.preprocess. RemoveType ( value , names=None , * , module=None , qualname=None , type=None , start=1 ) All methods of removing silence as attributes to get tab-completion and typo-proofing All All methods of removing silence as attributes to get tab-completion and typo-proofing Split All methods of removing silence as attributes to get tab-completion and typo-proofing Trim All methods of removing silence as attributes to get tab-completion and typo-proofing RemoveSilence class fastaudio.augment.preprocess. RemoveSilence ( self , remove_type= , threshold=20 , pad_ms=20 ) Split signal at points of silence greater than 2*pad_ms Resample class fastaudio.augment.preprocess. Resample ( self , sr_new ) Resample using faster polyphase technique and avoiding FFT computation","title":"augment.preprocess"},{"location":"API_Reference/augment.preprocess/#augmentpreprocess","text":"","title":"augment.preprocess"},{"location":"API_Reference/augment.preprocess/#removetype","text":"class fastaudio.augment.preprocess. RemoveType ( value , names=None , * , module=None , qualname=None , type=None , start=1 ) All methods of removing silence as attributes to get tab-completion and typo-proofing All All methods of removing silence as attributes to get tab-completion and typo-proofing Split All methods of removing silence as attributes to get tab-completion and typo-proofing Trim All methods of removing silence as attributes to get tab-completion and typo-proofing","title":"RemoveType"},{"location":"API_Reference/augment.preprocess/#removesilence","text":"class fastaudio.augment.preprocess. RemoveSilence ( self , remove_type= , threshold=20 , pad_ms=20 ) Split signal at points of silence greater than 2*pad_ms","title":"RemoveSilence"},{"location":"API_Reference/augment.preprocess/#resample","text":"class fastaudio.augment.preprocess. Resample ( self , sr_new ) Resample using faster polyphase technique and avoiding FFT computation","title":"Resample"},{"location":"API_Reference/augment.signal/","text":"augment.signal AudioPadType class fastaudio.augment.signal. AudioPadType ( value , names=None , * , module=None , qualname=None , type=None , start=1 ) An enumeration. Repeat An enumeration. Zeros An enumeration. Zeros_After An enumeration. ResizeSignal class fastaudio.augment.signal. ResizeSignal ( self , duration , pad_mode= ) Crops signal to be length specified in ms by duration, padding if needed SignalShifter class fastaudio.augment.signal. SignalShifter ( self , p=0.5 , max_pct=0.2 , max_time=None , direction=0 , roll=False ) Randomly shifts the audio signal by max_pct %. direction must be -1(left) 0(bidirectional) or 1(right). NoiseColor class fastaudio.augment.signal. NoiseColor ( ) Blue int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Brown int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Pink int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Violet int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 White int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 AddNoise class fastaudio.augment.signal. AddNoise ( self , noise_level=0.05 , color=0 ) Adds noise of specified color and level to the audio signal ChangeVolume class fastaudio.augment.signal. ChangeVolume ( self , p=0.5 , lower=0.5 , upper=1.5 ) Changes the volume of the signal SignalCutout class fastaudio.augment.signal. SignalCutout ( self , p=0.5 , max_cut_pct=0.15 ) Randomly zeros some portion of the signal SignalLoss class fastaudio.augment.signal. SignalLoss ( self , p=0.5 , max_loss_pct=0.15 ) Randomly loses some portion of the signal DownmixMono class fastaudio.augment.signal. DownmixMono ( self , enc=None , dec=None , split_idx=None , order=None ) Transform multichannel audios into single channel","title":"augment.signal"},{"location":"API_Reference/augment.signal/#augmentsignal","text":"","title":"augment.signal"},{"location":"API_Reference/augment.signal/#audiopadtype","text":"class fastaudio.augment.signal. AudioPadType ( value , names=None , * , module=None , qualname=None , type=None , start=1 ) An enumeration. Repeat An enumeration. Zeros An enumeration. Zeros_After An enumeration.","title":"AudioPadType"},{"location":"API_Reference/augment.signal/#resizesignal","text":"class fastaudio.augment.signal. ResizeSignal ( self , duration , pad_mode= ) Crops signal to be length specified in ms by duration, padding if needed","title":"ResizeSignal"},{"location":"API_Reference/augment.signal/#signalshifter","text":"class fastaudio.augment.signal. SignalShifter ( self , p=0.5 , max_pct=0.2 , max_time=None , direction=0 , roll=False ) Randomly shifts the audio signal by max_pct %. direction must be -1(left) 0(bidirectional) or 1(right).","title":"SignalShifter"},{"location":"API_Reference/augment.signal/#noisecolor","text":"class fastaudio.augment.signal. NoiseColor ( ) Blue int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Brown int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Pink int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Violet int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 White int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NoiseColor"},{"location":"API_Reference/augment.signal/#addnoise","text":"class fastaudio.augment.signal. AddNoise ( self , noise_level=0.05 , color=0 ) Adds noise of specified color and level to the audio signal","title":"AddNoise"},{"location":"API_Reference/augment.signal/#changevolume","text":"class fastaudio.augment.signal. ChangeVolume ( self , p=0.5 , lower=0.5 , upper=1.5 ) Changes the volume of the signal","title":"ChangeVolume"},{"location":"API_Reference/augment.signal/#signalcutout","text":"class fastaudio.augment.signal. SignalCutout ( self , p=0.5 , max_cut_pct=0.15 ) Randomly zeros some portion of the signal","title":"SignalCutout"},{"location":"API_Reference/augment.signal/#signalloss","text":"class fastaudio.augment.signal. SignalLoss ( self , p=0.5 , max_loss_pct=0.15 ) Randomly loses some portion of the signal","title":"SignalLoss"},{"location":"API_Reference/augment.signal/#downmixmono","text":"class fastaudio.augment.signal. DownmixMono ( self , enc=None , dec=None , split_idx=None , order=None ) Transform multichannel audios into single channel","title":"DownmixMono"},{"location":"API_Reference/augment.spectrogram/","text":"augment.spectrogram CropTime class fastaudio.augment.spectrogram. CropTime ( self , duration , pad_mode= ) Random crops full spectrogram to be length specified in ms by crop_duration MaskFreq class fastaudio.augment.spectrogram. MaskFreq ( self , num_masks=1 , size=20 , start=None , val=None ) Google SpecAugment frequency masking from https://arxiv.org/abs/1904.08779. MaskTime class fastaudio.augment.spectrogram. MaskTime ( self , num_masks=1 , size=20 , start=None , val=None ) Google SpecAugment time masking from https://arxiv.org/abs/1904.08779. SGRoll class fastaudio.augment.spectrogram. SGRoll ( self , max_shift_pct=0.5 , direction=0 ) Shifts spectrogram along x-axis wrapping around to other side Delta class fastaudio.augment.spectrogram. Delta ( self , width=9 ) Creates delta with order 1 and 2 from spectrogram and concatenate with the original TfmResize class fastaudio.augment.spectrogram. TfmResize ( self , size , interp_mode='bilinear' ) Temporary fix to allow image resizing transform","title":"augment.spectrogram"},{"location":"API_Reference/augment.spectrogram/#augmentspectrogram","text":"","title":"augment.spectrogram"},{"location":"API_Reference/augment.spectrogram/#croptime","text":"class fastaudio.augment.spectrogram. CropTime ( self , duration , pad_mode= ) Random crops full spectrogram to be length specified in ms by crop_duration","title":"CropTime"},{"location":"API_Reference/augment.spectrogram/#maskfreq","text":"class fastaudio.augment.spectrogram. MaskFreq ( self , num_masks=1 , size=20 , start=None , val=None ) Google SpecAugment frequency masking from https://arxiv.org/abs/1904.08779.","title":"MaskFreq"},{"location":"API_Reference/augment.spectrogram/#masktime","text":"class fastaudio.augment.spectrogram. MaskTime ( self , num_masks=1 , size=20 , start=None , val=None ) Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.","title":"MaskTime"},{"location":"API_Reference/augment.spectrogram/#sgroll","text":"class fastaudio.augment.spectrogram. SGRoll ( self , max_shift_pct=0.5 , direction=0 ) Shifts spectrogram along x-axis wrapping around to other side","title":"SGRoll"},{"location":"API_Reference/augment.spectrogram/#delta","text":"class fastaudio.augment.spectrogram. Delta ( self , width=9 ) Creates delta with order 1 and 2 from spectrogram and concatenate with the original","title":"Delta"},{"location":"API_Reference/augment.spectrogram/#tfmresize","text":"class fastaudio.augment.spectrogram. TfmResize ( self , size , interp_mode='bilinear' ) Temporary fix to allow image resizing transform","title":"TfmResize"},{"location":"API_Reference/core.config/","text":"core.config AudioBlock class fastaudio.core.config. AudioBlock ( cache_folder=None , sample_rate=16000 , force_mono=True , crop_signal_to=None ) A TransformBlock for audios from_folder ( path , sample_rate=16000 , force_mono=True , crop_signal_to=None , **kwargs ) Build a AudioBlock from a path and caches some intermediary results preprocess_audio_folder fastaudio.core.config. preprocess_audio_folder ( path , folders=None , output_dir=None , sample_rate=16000 , force_mono=True , crop_signal_to=None , **kwargs ) Preprocess audio files in path in parallel using n_workers PreprocessAudio class fastaudio.core.config. PreprocessAudio ( sample_rate=16000 , force_mono=True , crop_signal_to=None ) Creates an audio tensor and run the basic preprocessing transforms on it. Used while preprocessing the audios, this is not a Transform . AudioConfig class fastaudio.core.config. AudioConfig ( ) Collection of configurations to build AudioToSpec transforms. class BasicMFCC ( sample_rate=16000 , n_mfcc=40 , dct_type=2 , norm='ortho' , log_mels=False , melkwargs=None ) class BasicMelSpectrogram ( sample_rate=16000 , n_fft=400 , win_length=None , hop_length=None , f_min=0.0 , f_max=None , pad=0 , n_mels=128 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel=True , to_db=True ) class BasicSpectrogram ( n_fft=400 , win_length=None , hop_length=None , pad=0 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel=False , to_db=True ) class Voice ( sample_rate=16000 , n_fft=1024 , win_length=None , hop_length=128 , f_min=50.0 , f_max=8000.0 , pad=0 , n_mels=128 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel='True' , to_db='False' )","title":"core.config"},{"location":"API_Reference/core.config/#coreconfig","text":"","title":"core.config"},{"location":"API_Reference/core.config/#audioblock","text":"class fastaudio.core.config. AudioBlock ( cache_folder=None , sample_rate=16000 , force_mono=True , crop_signal_to=None ) A TransformBlock for audios from_folder ( path , sample_rate=16000 , force_mono=True , crop_signal_to=None , **kwargs ) Build a AudioBlock from a path and caches some intermediary results","title":"AudioBlock"},{"location":"API_Reference/core.config/#preprocess_audio_folder","text":"fastaudio.core.config. preprocess_audio_folder ( path , folders=None , output_dir=None , sample_rate=16000 , force_mono=True , crop_signal_to=None , **kwargs ) Preprocess audio files in path in parallel using n_workers","title":"preprocess_audio_folder"},{"location":"API_Reference/core.config/#preprocessaudio","text":"class fastaudio.core.config. PreprocessAudio ( sample_rate=16000 , force_mono=True , crop_signal_to=None ) Creates an audio tensor and run the basic preprocessing transforms on it. Used while preprocessing the audios, this is not a Transform .","title":"PreprocessAudio"},{"location":"API_Reference/core.config/#audioconfig","text":"class fastaudio.core.config. AudioConfig ( ) Collection of configurations to build AudioToSpec transforms. class BasicMFCC ( sample_rate=16000 , n_mfcc=40 , dct_type=2 , norm='ortho' , log_mels=False , melkwargs=None ) class BasicMelSpectrogram ( sample_rate=16000 , n_fft=400 , win_length=None , hop_length=None , f_min=0.0 , f_max=None , pad=0 , n_mels=128 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel=True , to_db=True ) class BasicSpectrogram ( n_fft=400 , win_length=None , hop_length=None , pad=0 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel=False , to_db=True ) class Voice ( sample_rate=16000 , n_fft=1024 , win_length=None , hop_length=128 , f_min=50.0 , f_max=8000.0 , pad=0 , n_mels=128 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel='True' , to_db='False' )","title":"AudioConfig"},{"location":"API_Reference/core.signal/","text":"core.signal audio_extensions fastaudio.core.signal. audio_extensions List of extensions suitable for audio files get_audio_files fastaudio.core.signal. get_audio_files ( path , recurse=True , folders=None ) Get audio files in path recursively, only in folders , if specified. AudioGetter fastaudio.core.signal. AudioGetter ( suf='' , recurse=True , folders=None ) Create get_audio_files partial function that searches path suffix suf and passes along kwargs , only in folders , if specified. tar_extract_at_filename fastaudio.core.signal. tar_extract_at_filename ( fname , dest ) Extract fname to dest / fname.name folder using tarfile AudioTensor class fastaudio.core.signal. AudioTensor ( x , sr=None , **kwargs ) Semantic torch tensor that represents an audio. Contains all of the functionality of a normal tensor, but additionally can be created from files and has extra properties. Also knows how to show itself. create ( fn , cache_folder=None , out=None , normalization=True , channels_first=True , num_frames=0 , offset=0 , signalinfo=None , encodinginfo=None , filetype=None , **kwargs ) Creates audio tensor from file sr Property. Sampling rate of the audio nsamples nchannels duration hear ( self ) Listen to audio clip. Creates a html player. show ( self , ctx=None , hear=True , **kwargs ) Show audio clip using librosa. Pass hear=True to also display a html player to listen. OpenAudio class fastaudio.core.signal. OpenAudio ( self , items ) Transform that creates AudioTensors from a list of files.","title":"core.signal"},{"location":"API_Reference/core.signal/#coresignal","text":"","title":"core.signal"},{"location":"API_Reference/core.signal/#audio_extensions","text":"fastaudio.core.signal. audio_extensions List of extensions suitable for audio files","title":"audio_extensions"},{"location":"API_Reference/core.signal/#get_audio_files","text":"fastaudio.core.signal. get_audio_files ( path , recurse=True , folders=None ) Get audio files in path recursively, only in folders , if specified.","title":"get_audio_files"},{"location":"API_Reference/core.signal/#audiogetter","text":"fastaudio.core.signal. AudioGetter ( suf='' , recurse=True , folders=None ) Create get_audio_files partial function that searches path suffix suf and passes along kwargs , only in folders , if specified.","title":"AudioGetter"},{"location":"API_Reference/core.signal/#tar_extract_at_filename","text":"fastaudio.core.signal. tar_extract_at_filename ( fname , dest ) Extract fname to dest / fname.name folder using tarfile","title":"tar_extract_at_filename"},{"location":"API_Reference/core.signal/#audiotensor","text":"class fastaudio.core.signal. AudioTensor ( x , sr=None , **kwargs ) Semantic torch tensor that represents an audio. Contains all of the functionality of a normal tensor, but additionally can be created from files and has extra properties. Also knows how to show itself. create ( fn , cache_folder=None , out=None , normalization=True , channels_first=True , num_frames=0 , offset=0 , signalinfo=None , encodinginfo=None , filetype=None , **kwargs ) Creates audio tensor from file sr Property. Sampling rate of the audio nsamples nchannels duration hear ( self ) Listen to audio clip. Creates a html player. show ( self , ctx=None , hear=True , **kwargs ) Show audio clip using librosa. Pass hear=True to also display a html player to listen.","title":"AudioTensor"},{"location":"API_Reference/core.signal/#openaudio","text":"class fastaudio.core.signal. OpenAudio ( self , items ) Transform that creates AudioTensors from a list of files.","title":"OpenAudio"},{"location":"API_Reference/core.spectrogram/","text":"core.spectrogram AudioSpectrogram class fastaudio.core.spectrogram. AudioSpectrogram ( x , **kwargs ) Semantic torch tensor that represents an Audio Spectrogram. Contains all of the functionality of a normal tensor, but has extra properties and knows how to show itself. create ( sg_tensor , settings=None ) Create an AudioSpectrogram from a torch tensor duration height width show ( self , ctx=None , ax=None , title='' , **kwargs ) Show spectrogram using librosa AudioToSpec class fastaudio.core.spectrogram. AudioToSpec ( self , pipe , settings ) Transform to create spectrograms from audio tensors. from_cfg ( audio_cfg ) Creates AudioToSpec from configuration file SpectrogramTransformer fastaudio.core.spectrogram. SpectrogramTransformer ( mel=True , to_db=True ) Creates a factory for creating AudioToSpec transforms with different parameters AudioToMFCC class fastaudio.core.spectrogram. AudioToMFCC ( self , sample_rate=16000 , n_mfcc=40 , dct_type=2 , norm='ortho' , log_mels=False , melkwargs=None ) Transform to create MFCC features from audio tensors. from_cfg ( audio_cfg ) Creates AudioToMFCC from configuration file","title":"core.spectrogram"},{"location":"API_Reference/core.spectrogram/#corespectrogram","text":"","title":"core.spectrogram"},{"location":"API_Reference/core.spectrogram/#audiospectrogram","text":"class fastaudio.core.spectrogram. AudioSpectrogram ( x , **kwargs ) Semantic torch tensor that represents an Audio Spectrogram. Contains all of the functionality of a normal tensor, but has extra properties and knows how to show itself. create ( sg_tensor , settings=None ) Create an AudioSpectrogram from a torch tensor duration height width show ( self , ctx=None , ax=None , title='' , **kwargs ) Show spectrogram using librosa","title":"AudioSpectrogram"},{"location":"API_Reference/core.spectrogram/#audiotospec","text":"class fastaudio.core.spectrogram. AudioToSpec ( self , pipe , settings ) Transform to create spectrograms from audio tensors. from_cfg ( audio_cfg ) Creates AudioToSpec from configuration file","title":"AudioToSpec"},{"location":"API_Reference/core.spectrogram/#spectrogramtransformer","text":"fastaudio.core.spectrogram. SpectrogramTransformer ( mel=True , to_db=True ) Creates a factory for creating AudioToSpec transforms with different parameters","title":"SpectrogramTransformer"},{"location":"API_Reference/core.spectrogram/#audiotomfcc","text":"class fastaudio.core.spectrogram. AudioToMFCC ( self , sample_rate=16000 , n_mfcc=40 , dct_type=2 , norm='ortho' , log_mels=False , melkwargs=None ) Transform to create MFCC features from audio tensors. from_cfg ( audio_cfg ) Creates AudioToMFCC from configuration file","title":"AudioToMFCC"}]}
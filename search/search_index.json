{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fastaudio An audio module for fastai v2. We want to help you build audio machine learning applications while minimizing the need for audio domain expertise. Currently under development. Quick Start Google Colab Notebook Zachary Mueller's class","title":"Fastaudio"},{"location":"#fastaudio","text":"An audio module for fastai v2. We want to help you build audio machine learning applications while minimizing the need for audio domain expertise. Currently under development.","title":"Fastaudio"},{"location":"#quick-start","text":"Google Colab Notebook Zachary Mueller's class","title":"Quick Start"},{"location":"Training_tutorial/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Simple training tutorial The objective of this tutorial is to show you the basics of the library and how it can be used to simplify the audio processing pipeline. This page is generated from the corresponding jupyter notebook, that can be found on this folder To install the library, just run this cell: ! pip install git + https : // github . com / fastaudio / fastaudio . git COLAB USERS: Before you continue and import the lib, go to the Runtime menu and select Restart Runtime . from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * ESC-50: Dataset for Environmental Sound Classification #The first time this will download a dataset that is ~650mb path = untar_data ( URLs . ESC50 , dest = \"ESC50\" ) The audio files are inside a subfolder audio/ ( path / \"audio\" ) . ls () (#2000) [Path('/home/scart/.fastai/data/master/audio/5-198891-A-8.wav'),Path('/home/scart/.fastai/data/master/audio/3-128512-A-47.wav'),Path('/home/scart/.fastai/data/master/audio/4-234879-A-6.wav'),Path('/home/scart/.fastai/data/master/audio/3-100024-A-27.wav'),Path('/home/scart/.fastai/data/master/audio/5-263831-A-6.wav'),Path('/home/scart/.fastai/data/master/audio/1-22804-A-46.wav'),Path('/home/scart/.fastai/data/master/audio/2-117615-A-48.wav'),Path('/home/scart/.fastai/data/master/audio/5-221518-A-21.wav'),Path('/home/scart/.fastai/data/master/audio/2-43802-A-42.wav'),Path('/home/scart/.fastai/data/master/audio/5-194899-D-3.wav')...] And there's another folder meta/ with some metadata about all the files and the labels ( path / \"meta\" ) . ls () (#2) [Path('/home/scart/.fastai/data/master/meta/esc50.csv'),Path('/home/scart/.fastai/data/master/meta/esc50-human.xlsx')] Opening the metadata file df = pd . read_csv ( path / \"meta\" / \"esc50.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } filename fold target category esc10 src_file take 0 1-100032-A-0.wav 1 0 dog True 100032 A 1 1-100038-A-14.wav 1 14 chirping_birds False 100038 A 2 1-100210-A-36.wav 1 36 vacuum_cleaner False 100210 A 3 1-100210-B-36.wav 1 36 vacuum_cleaner False 100210 B 4 1-101296-A-19.wav 1 19 thunderstorm False 101296 A Datablock and Basic End to End Training # Helper function to split the data def CrossValidationSplitter ( col = 'fold' , fold = 1 ): \"Split `items` (supposed to be a dataframe) by fold in `col`\" def _inner ( o ): assert isinstance ( o , pd . DataFrame ), \"ColSplitter only works when your items are a pandas DataFrame\" col_values = o . iloc [:, col ] if isinstance ( col , int ) else o [ col ] valid_idx = ( col_values == fold ) . values . astype ( 'bool' ) return IndexSplitter ( mask2idxs ( valid_idx ))( o ) return _inner Creating the Audio to Spectrogram transform from a predefined config. cfg = AudioConfig . BasicMelSpectrogram ( n_fft = 512 ) a2s = AudioToSpec . from_cfg ( cfg ) Creating the Datablock auds = DataBlock ( blocks = ( AudioBlock , CategoryBlock ), get_x = ColReader ( \"filename\" , pref = path / \"audio\" ), splitter = CrossValidationSplitter ( fold = 1 ), batch_tfms = [ a2s ], get_y = ColReader ( \"category\" )) dbunch = auds . dataloaders ( df , bs = 64 ) Visualizing one batch of data. Notice that the title of each Spectrogram is the corresponding label. dbunch . show_batch ( figsize = ( 10 , 5 )) Learner and Training While creating the learner, we need to pass a special cnn_config to indicate that our input spectrograms only have one channel. Besides that, it's the usual vision learner. learn = cnn_learner ( dbunch , resnet18 , config = cnn_config ( n_in = 1 ), #<- Only audio specific modification here loss_fn = CrossEntropyLossFlat , metrics = [ accuracy ]) learn . fine_tune ( 10 ) epoch train_loss valid_loss accuracy time 0 4.559088 2.335686 0.340000 00:09 epoch train_loss valid_loss accuracy time 0 2.299654 1.825524 0.497500 00:09 1 1.779437 1.425651 0.590000 00:09 2 1.301816 1.242665 0.642500 00:09 3 0.882593 1.107010 0.692500 00:09 4 0.596167 1.014652 0.707500 00:09 5 0.401981 0.997006 0.720000 00:09 6 0.282515 1.003401 0.727500 00:09 7 0.197506 0.973345 0.740000 00:09 8 0.144901 0.970970 0.727500 00:09 9 0.108945 0.977597 0.727500 00:09","title":"Training tutorial"},{"location":"Training_tutorial/#simple-training-tutorial","text":"The objective of this tutorial is to show you the basics of the library and how it can be used to simplify the audio processing pipeline. This page is generated from the corresponding jupyter notebook, that can be found on this folder To install the library, just run this cell: ! pip install git + https : // github . com / fastaudio / fastaudio . git COLAB USERS: Before you continue and import the lib, go to the Runtime menu and select Restart Runtime . from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import *","title":"Simple training tutorial"},{"location":"Training_tutorial/#esc-50-dataset-for-environmental-sound-classification","text":"#The first time this will download a dataset that is ~650mb path = untar_data ( URLs . ESC50 , dest = \"ESC50\" ) The audio files are inside a subfolder audio/ ( path / \"audio\" ) . ls () (#2000) [Path('/home/scart/.fastai/data/master/audio/5-198891-A-8.wav'),Path('/home/scart/.fastai/data/master/audio/3-128512-A-47.wav'),Path('/home/scart/.fastai/data/master/audio/4-234879-A-6.wav'),Path('/home/scart/.fastai/data/master/audio/3-100024-A-27.wav'),Path('/home/scart/.fastai/data/master/audio/5-263831-A-6.wav'),Path('/home/scart/.fastai/data/master/audio/1-22804-A-46.wav'),Path('/home/scart/.fastai/data/master/audio/2-117615-A-48.wav'),Path('/home/scart/.fastai/data/master/audio/5-221518-A-21.wav'),Path('/home/scart/.fastai/data/master/audio/2-43802-A-42.wav'),Path('/home/scart/.fastai/data/master/audio/5-194899-D-3.wav')...] And there's another folder meta/ with some metadata about all the files and the labels ( path / \"meta\" ) . ls () (#2) [Path('/home/scart/.fastai/data/master/meta/esc50.csv'),Path('/home/scart/.fastai/data/master/meta/esc50-human.xlsx')] Opening the metadata file df = pd . read_csv ( path / \"meta\" / \"esc50.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } filename fold target category esc10 src_file take 0 1-100032-A-0.wav 1 0 dog True 100032 A 1 1-100038-A-14.wav 1 14 chirping_birds False 100038 A 2 1-100210-A-36.wav 1 36 vacuum_cleaner False 100210 A 3 1-100210-B-36.wav 1 36 vacuum_cleaner False 100210 B 4 1-101296-A-19.wav 1 19 thunderstorm False 101296 A","title":"ESC-50: Dataset for Environmental Sound Classification"},{"location":"Training_tutorial/#datablock-and-basic-end-to-end-training","text":"# Helper function to split the data def CrossValidationSplitter ( col = 'fold' , fold = 1 ): \"Split `items` (supposed to be a dataframe) by fold in `col`\" def _inner ( o ): assert isinstance ( o , pd . DataFrame ), \"ColSplitter only works when your items are a pandas DataFrame\" col_values = o . iloc [:, col ] if isinstance ( col , int ) else o [ col ] valid_idx = ( col_values == fold ) . values . astype ( 'bool' ) return IndexSplitter ( mask2idxs ( valid_idx ))( o ) return _inner Creating the Audio to Spectrogram transform from a predefined config. cfg = AudioConfig . BasicMelSpectrogram ( n_fft = 512 ) a2s = AudioToSpec . from_cfg ( cfg ) Creating the Datablock auds = DataBlock ( blocks = ( AudioBlock , CategoryBlock ), get_x = ColReader ( \"filename\" , pref = path / \"audio\" ), splitter = CrossValidationSplitter ( fold = 1 ), batch_tfms = [ a2s ], get_y = ColReader ( \"category\" )) dbunch = auds . dataloaders ( df , bs = 64 ) Visualizing one batch of data. Notice that the title of each Spectrogram is the corresponding label. dbunch . show_batch ( figsize = ( 10 , 5 ))","title":"Datablock and Basic End to End Training"},{"location":"Training_tutorial/#learner-and-training","text":"While creating the learner, we need to pass a special cnn_config to indicate that our input spectrograms only have one channel. Besides that, it's the usual vision learner. learn = cnn_learner ( dbunch , resnet18 , config = cnn_config ( n_in = 1 ), #<- Only audio specific modification here loss_fn = CrossEntropyLossFlat , metrics = [ accuracy ]) learn . fine_tune ( 10 ) epoch train_loss valid_loss accuracy time 0 4.559088 2.335686 0.340000 00:09 epoch train_loss valid_loss accuracy time 0 2.299654 1.825524 0.497500 00:09 1 1.779437 1.425651 0.590000 00:09 2 1.301816 1.242665 0.642500 00:09 3 0.882593 1.107010 0.692500 00:09 4 0.596167 1.014652 0.707500 00:09 5 0.401981 0.997006 0.720000 00:09 6 0.282515 1.003401 0.727500 00:09 7 0.197506 0.973345 0.740000 00:09 8 0.144901 0.970970 0.727500 00:09 9 0.108945 0.977597 0.727500 00:09","title":"Learner and Training"},{"location":"API_Reference/augment.preprocess/","text":"augment.preprocess RemoveType class fastaudio.augment.preprocess. RemoveType ( *args , **kwargs ) All methods of removing silence as attributes to get tab-completion and typo-proofing All str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. Split str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. Trim str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. RemoveSilence class fastaudio.augment.preprocess. RemoveSilence ( self , remove_type='trim' , threshold=20 , pad_ms=20 ) Split signal at points of silence greater than 2*pad_ms Resample class fastaudio.augment.preprocess. Resample ( self , sr_new ) Resample using faster polyphase technique and avoiding FFT computation","title":"augment.preprocess"},{"location":"API_Reference/augment.preprocess/#augmentpreprocess","text":"","title":"augment.preprocess"},{"location":"API_Reference/augment.preprocess/#removetype","text":"class fastaudio.augment.preprocess. RemoveType ( *args , **kwargs ) All methods of removing silence as attributes to get tab-completion and typo-proofing All str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. Split str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. Trim str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"RemoveType"},{"location":"API_Reference/augment.preprocess/#removesilence","text":"class fastaudio.augment.preprocess. RemoveSilence ( self , remove_type='trim' , threshold=20 , pad_ms=20 ) Split signal at points of silence greater than 2*pad_ms","title":"RemoveSilence"},{"location":"API_Reference/augment.preprocess/#resample","text":"class fastaudio.augment.preprocess. Resample ( self , sr_new ) Resample using faster polyphase technique and avoiding FFT computation","title":"Resample"},{"location":"API_Reference/augment.signal/","text":"augment.signal AudioPadType class fastaudio.augment.signal. AudioPadType ( *args , **kwargs ) All methods of padding audio as attributes to get tab-completion and typo-proofing Repeat str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. Zeros str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. Zeros_After str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. ResizeSignal class fastaudio.augment.signal. ResizeSignal ( self , duration , pad_mode='zeros' ) Crops signal to be length specified in ms by duration, padding if needed SignalShifter class fastaudio.augment.signal. SignalShifter ( self , p=0.5 , max_pct=0.2 , max_time=None , direction=0 , roll=False ) Randomly shifts the audio signal by max_pct %. direction must be -1(left) 0(bidirectional) or 1(right). NoiseColor class fastaudio.augment.signal. NoiseColor ( *args , **kwargs ) All possible colors of noise as attributes to get tab-completion and typo-proofing Blue int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Brown int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Pink int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Violet int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 White int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 AddNoise class fastaudio.augment.signal. AddNoise ( self , noise_level=0.05 , color=0 ) Adds noise of specified color and level to the audio signal ChangeVolume class fastaudio.augment.signal. ChangeVolume ( self , p=0.5 , lower=0.5 , upper=1.5 ) Changes the volume of the signal SignalCutout class fastaudio.augment.signal. SignalCutout ( self , p=0.5 , max_cut_pct=0.15 ) Randomly zeros some portion of the signal SignalLoss class fastaudio.augment.signal. SignalLoss ( self , p=0.5 , max_loss_pct=0.15 ) Randomly loses some portion of the signal DownmixMono class fastaudio.augment.signal. DownmixMono ( self , enc=None , dec=None , split_idx=None , order=None ) Transform multichannel audios into single channel","title":"augment.signal"},{"location":"API_Reference/augment.signal/#augmentsignal","text":"","title":"augment.signal"},{"location":"API_Reference/augment.signal/#audiopadtype","text":"class fastaudio.augment.signal. AudioPadType ( *args , **kwargs ) All methods of padding audio as attributes to get tab-completion and typo-proofing Repeat str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. Zeros str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'. Zeros_After str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object. str () (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.","title":"AudioPadType"},{"location":"API_Reference/augment.signal/#resizesignal","text":"class fastaudio.augment.signal. ResizeSignal ( self , duration , pad_mode='zeros' ) Crops signal to be length specified in ms by duration, padding if needed","title":"ResizeSignal"},{"location":"API_Reference/augment.signal/#signalshifter","text":"class fastaudio.augment.signal. SignalShifter ( self , p=0.5 , max_pct=0.2 , max_time=None , direction=0 , roll=False ) Randomly shifts the audio signal by max_pct %. direction must be -1(left) 0(bidirectional) or 1(right).","title":"SignalShifter"},{"location":"API_Reference/augment.signal/#noisecolor","text":"class fastaudio.augment.signal. NoiseColor ( *args , **kwargs ) All possible colors of noise as attributes to get tab-completion and typo-proofing Blue int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Brown int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Pink int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 Violet int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4 White int([x]) -> integer int(x, base=10) -> integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x. int (). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by '+' or '-' and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. int('0b100', base=0) 4","title":"NoiseColor"},{"location":"API_Reference/augment.signal/#addnoise","text":"class fastaudio.augment.signal. AddNoise ( self , noise_level=0.05 , color=0 ) Adds noise of specified color and level to the audio signal","title":"AddNoise"},{"location":"API_Reference/augment.signal/#changevolume","text":"class fastaudio.augment.signal. ChangeVolume ( self , p=0.5 , lower=0.5 , upper=1.5 ) Changes the volume of the signal","title":"ChangeVolume"},{"location":"API_Reference/augment.signal/#signalcutout","text":"class fastaudio.augment.signal. SignalCutout ( self , p=0.5 , max_cut_pct=0.15 ) Randomly zeros some portion of the signal","title":"SignalCutout"},{"location":"API_Reference/augment.signal/#signalloss","text":"class fastaudio.augment.signal. SignalLoss ( self , p=0.5 , max_loss_pct=0.15 ) Randomly loses some portion of the signal","title":"SignalLoss"},{"location":"API_Reference/augment.signal/#downmixmono","text":"class fastaudio.augment.signal. DownmixMono ( self , enc=None , dec=None , split_idx=None , order=None ) Transform multichannel audios into single channel","title":"DownmixMono"},{"location":"API_Reference/augment.spectrogram/","text":"augment.spectrogram CropTime class fastaudio.augment.spectrogram. CropTime ( self , duration , pad_mode='zeros' ) Random crops full spectrogram to be length specified in ms by crop_duration MaskFreq class fastaudio.augment.spectrogram. MaskFreq ( self , num_masks=1 , size=20 , start=None , val=None ) Google SpecAugment frequency masking from https://arxiv.org/abs/1904.08779. MaskTime class fastaudio.augment.spectrogram. MaskTime ( self , num_masks=1 , size=20 , start=None , val=None ) Google SpecAugment time masking from https://arxiv.org/abs/1904.08779. SGRoll class fastaudio.augment.spectrogram. SGRoll ( self , max_shift_pct=0.5 , direction=0 ) Shifts spectrogram along x-axis wrapping around to other side Delta class fastaudio.augment.spectrogram. Delta ( self , width=9 ) Creates delta with order 1 and 2 from spectrogram and concatenate with the original TfmResize class fastaudio.augment.spectrogram. TfmResize ( self , size , interp_mode='bilinear' ) Temporary fix to allow image resizing transform","title":"augment.spectrogram"},{"location":"API_Reference/augment.spectrogram/#augmentspectrogram","text":"","title":"augment.spectrogram"},{"location":"API_Reference/augment.spectrogram/#croptime","text":"class fastaudio.augment.spectrogram. CropTime ( self , duration , pad_mode='zeros' ) Random crops full spectrogram to be length specified in ms by crop_duration","title":"CropTime"},{"location":"API_Reference/augment.spectrogram/#maskfreq","text":"class fastaudio.augment.spectrogram. MaskFreq ( self , num_masks=1 , size=20 , start=None , val=None ) Google SpecAugment frequency masking from https://arxiv.org/abs/1904.08779.","title":"MaskFreq"},{"location":"API_Reference/augment.spectrogram/#masktime","text":"class fastaudio.augment.spectrogram. MaskTime ( self , num_masks=1 , size=20 , start=None , val=None ) Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.","title":"MaskTime"},{"location":"API_Reference/augment.spectrogram/#sgroll","text":"class fastaudio.augment.spectrogram. SGRoll ( self , max_shift_pct=0.5 , direction=0 ) Shifts spectrogram along x-axis wrapping around to other side","title":"SGRoll"},{"location":"API_Reference/augment.spectrogram/#delta","text":"class fastaudio.augment.spectrogram. Delta ( self , width=9 ) Creates delta with order 1 and 2 from spectrogram and concatenate with the original","title":"Delta"},{"location":"API_Reference/augment.spectrogram/#tfmresize","text":"class fastaudio.augment.spectrogram. TfmResize ( self , size , interp_mode='bilinear' ) Temporary fix to allow image resizing transform","title":"TfmResize"},{"location":"API_Reference/core.config/","text":"core.config AudioBlock class fastaudio.core.config. AudioBlock ( cache_folder=None , sample_rate=16000 , force_mono=True , crop_signal_to=None ) A TransformBlock for audios from_folder ( path , sample_rate=16000 , force_mono=True , crop_signal_to=None , **kwargs ) Build a AudioBlock from a path and caches some intermediary results preprocess_audio_folder fastaudio.core.config. preprocess_audio_folder ( path , folders=None , output_dir=None , sample_rate=16000 , force_mono=True , crop_signal_to=None , **kwargs ) Preprocess audio files in path in parallel using n_workers PreprocessAudio class fastaudio.core.config. PreprocessAudio ( sample_rate=16000 , force_mono=True , crop_signal_to=None ) Creates an audio tensor and run the basic preprocessing transforms on it. Used while preprocessing the audios, this is not a Transform . AudioConfig class fastaudio.core.config. AudioConfig ( ) Collection of configurations to build AudioToSpec transforms. class BasicMFCC ( sample_rate=16000 , n_mfcc=40 , dct_type=2 , norm='ortho' , log_mels=False , melkwargs=None ) class BasicMelSpectrogram ( sample_rate=16000 , n_fft=400 , win_length=None , hop_length=None , f_min=0.0 , f_max=None , pad=0 , n_mels=128 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel=True , to_db=True ) class BasicSpectrogram ( n_fft=400 , win_length=None , hop_length=None , pad=0 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel=False , to_db=True ) class Voice ( sample_rate=16000 , n_fft=1024 , win_length=None , hop_length=128 , f_min=50.0 , f_max=8000.0 , pad=0 , n_mels=128 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel='True' , to_db='False' )","title":"core.config"},{"location":"API_Reference/core.config/#coreconfig","text":"","title":"core.config"},{"location":"API_Reference/core.config/#audioblock","text":"class fastaudio.core.config. AudioBlock ( cache_folder=None , sample_rate=16000 , force_mono=True , crop_signal_to=None ) A TransformBlock for audios from_folder ( path , sample_rate=16000 , force_mono=True , crop_signal_to=None , **kwargs ) Build a AudioBlock from a path and caches some intermediary results","title":"AudioBlock"},{"location":"API_Reference/core.config/#preprocess_audio_folder","text":"fastaudio.core.config. preprocess_audio_folder ( path , folders=None , output_dir=None , sample_rate=16000 , force_mono=True , crop_signal_to=None , **kwargs ) Preprocess audio files in path in parallel using n_workers","title":"preprocess_audio_folder"},{"location":"API_Reference/core.config/#preprocessaudio","text":"class fastaudio.core.config. PreprocessAudio ( sample_rate=16000 , force_mono=True , crop_signal_to=None ) Creates an audio tensor and run the basic preprocessing transforms on it. Used while preprocessing the audios, this is not a Transform .","title":"PreprocessAudio"},{"location":"API_Reference/core.config/#audioconfig","text":"class fastaudio.core.config. AudioConfig ( ) Collection of configurations to build AudioToSpec transforms. class BasicMFCC ( sample_rate=16000 , n_mfcc=40 , dct_type=2 , norm='ortho' , log_mels=False , melkwargs=None ) class BasicMelSpectrogram ( sample_rate=16000 , n_fft=400 , win_length=None , hop_length=None , f_min=0.0 , f_max=None , pad=0 , n_mels=128 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel=True , to_db=True ) class BasicSpectrogram ( n_fft=400 , win_length=None , hop_length=None , pad=0 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel=False , to_db=True ) class Voice ( sample_rate=16000 , n_fft=1024 , win_length=None , hop_length=128 , f_min=50.0 , f_max=8000.0 , pad=0 , n_mels=128 , window_fn= , power=2.0 , normalized=False , wkwargs=None , mel='True' , to_db='False' )","title":"AudioConfig"},{"location":"API_Reference/core.signal/","text":"core.signal audio_extensions fastaudio.core.signal. audio_extensions List of extensions suitable for audio files get_audio_files fastaudio.core.signal. get_audio_files ( path , recurse=True , folders=None ) Get audio files in path recursively, only in folders , if specified. AudioGetter fastaudio.core.signal. AudioGetter ( suf='' , recurse=True , folders=None ) Create get_audio_files partial function that searches path suffix suf and passes along kwargs , only in folders , if specified. tar_extract_at_filename fastaudio.core.signal. tar_extract_at_filename ( fname , dest ) Extract fname to dest / fname.name folder using tarfile AudioTensor class fastaudio.core.signal. AudioTensor ( x , sr=None , **kwargs ) Semantic torch tensor that represents an audio. Contains all of the functionality of a normal tensor, but additionally can be created from files and has extra properties. Also knows how to show itself. create ( fn , cache_folder=None , out=None , normalization=True , channels_first=True , num_frames=0 , offset=0 , signalinfo=None , encodinginfo=None , filetype=None , **kwargs ) Creates audio tensor from file sr Property. Sampling rate of the audio nsamples partial(func, args, *keywords) - new function with partial application of the given arguments and keywords. nchannels partial(func, args, *keywords) - new function with partial application of the given arguments and keywords. duration hear ( self ) Listen to audio clip. Creates a html player. show ( self , ctx=None , hear=True , **kwargs ) Show audio clip using librosa. Pass hear=True to also display a html player to listen. OpenAudio class fastaudio.core.signal. OpenAudio ( self , items ) Transform that creates AudioTensors from a list of files.","title":"core.signal"},{"location":"API_Reference/core.signal/#coresignal","text":"","title":"core.signal"},{"location":"API_Reference/core.signal/#audio_extensions","text":"fastaudio.core.signal. audio_extensions List of extensions suitable for audio files","title":"audio_extensions"},{"location":"API_Reference/core.signal/#get_audio_files","text":"fastaudio.core.signal. get_audio_files ( path , recurse=True , folders=None ) Get audio files in path recursively, only in folders , if specified.","title":"get_audio_files"},{"location":"API_Reference/core.signal/#audiogetter","text":"fastaudio.core.signal. AudioGetter ( suf='' , recurse=True , folders=None ) Create get_audio_files partial function that searches path suffix suf and passes along kwargs , only in folders , if specified.","title":"AudioGetter"},{"location":"API_Reference/core.signal/#tar_extract_at_filename","text":"fastaudio.core.signal. tar_extract_at_filename ( fname , dest ) Extract fname to dest / fname.name folder using tarfile","title":"tar_extract_at_filename"},{"location":"API_Reference/core.signal/#audiotensor","text":"class fastaudio.core.signal. AudioTensor ( x , sr=None , **kwargs ) Semantic torch tensor that represents an audio. Contains all of the functionality of a normal tensor, but additionally can be created from files and has extra properties. Also knows how to show itself. create ( fn , cache_folder=None , out=None , normalization=True , channels_first=True , num_frames=0 , offset=0 , signalinfo=None , encodinginfo=None , filetype=None , **kwargs ) Creates audio tensor from file sr Property. Sampling rate of the audio nsamples partial(func, args, *keywords) - new function with partial application of the given arguments and keywords. nchannels partial(func, args, *keywords) - new function with partial application of the given arguments and keywords. duration hear ( self ) Listen to audio clip. Creates a html player. show ( self , ctx=None , hear=True , **kwargs ) Show audio clip using librosa. Pass hear=True to also display a html player to listen.","title":"AudioTensor"},{"location":"API_Reference/core.signal/#openaudio","text":"class fastaudio.core.signal. OpenAudio ( self , items ) Transform that creates AudioTensors from a list of files.","title":"OpenAudio"},{"location":"API_Reference/core.spectrogram/","text":"core.spectrogram AudioSpectrogram class fastaudio.core.spectrogram. AudioSpectrogram ( x , **kwargs ) Semantic torch tensor that represents an Audio Spectrogram. Contains all of the functionality of a normal tensor, but has extra properties and knows how to show itself. create ( sg_tensor , settings=None ) Create an AudioSpectrogram from a torch tensor duration height partial(func, args, *keywords) - new function with partial application of the given arguments and keywords. width partial(func, args, *keywords) - new function with partial application of the given arguments and keywords. show ( self , ctx=None , ax=None , title='' , **kwargs ) Show spectrogram using librosa AudioToSpec class fastaudio.core.spectrogram. AudioToSpec ( self , pipe , settings ) Transform to create spectrograms from audio tensors. from_cfg ( audio_cfg ) Creates AudioToSpec from configuration file SpectrogramTransformer fastaudio.core.spectrogram. SpectrogramTransformer ( mel=True , to_db=True ) Creates a factory for creating AudioToSpec transforms with different parameters AudioToMFCC class fastaudio.core.spectrogram. AudioToMFCC ( self , sample_rate=16000 , n_mfcc=40 , dct_type=2 , norm='ortho' , log_mels=False , melkwargs=None ) Transform to create MFCC features from audio tensors. from_cfg ( audio_cfg ) Creates AudioToMFCC from configuration file","title":"core.spectrogram"},{"location":"API_Reference/core.spectrogram/#corespectrogram","text":"","title":"core.spectrogram"},{"location":"API_Reference/core.spectrogram/#audiospectrogram","text":"class fastaudio.core.spectrogram. AudioSpectrogram ( x , **kwargs ) Semantic torch tensor that represents an Audio Spectrogram. Contains all of the functionality of a normal tensor, but has extra properties and knows how to show itself. create ( sg_tensor , settings=None ) Create an AudioSpectrogram from a torch tensor duration height partial(func, args, *keywords) - new function with partial application of the given arguments and keywords. width partial(func, args, *keywords) - new function with partial application of the given arguments and keywords. show ( self , ctx=None , ax=None , title='' , **kwargs ) Show spectrogram using librosa","title":"AudioSpectrogram"},{"location":"API_Reference/core.spectrogram/#audiotospec","text":"class fastaudio.core.spectrogram. AudioToSpec ( self , pipe , settings ) Transform to create spectrograms from audio tensors. from_cfg ( audio_cfg ) Creates AudioToSpec from configuration file","title":"AudioToSpec"},{"location":"API_Reference/core.spectrogram/#spectrogramtransformer","text":"fastaudio.core.spectrogram. SpectrogramTransformer ( mel=True , to_db=True ) Creates a factory for creating AudioToSpec transforms with different parameters","title":"SpectrogramTransformer"},{"location":"API_Reference/core.spectrogram/#audiotomfcc","text":"class fastaudio.core.spectrogram. AudioToMFCC ( self , sample_rate=16000 , n_mfcc=40 , dct_type=2 , norm='ortho' , log_mels=False , melkwargs=None ) Transform to create MFCC features from audio tensors. from_cfg ( audio_cfg ) Creates AudioToMFCC from configuration file","title":"AudioToMFCC"}]}